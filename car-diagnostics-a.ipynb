{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-22T12:31:24.284987Z","iopub.execute_input":"2023-03-22T12:31:24.286450Z","iopub.status.idle":"2023-03-22T12:31:24.304175Z","shell.execute_reply.started":"2023-03-22T12:31:24.286394Z","shell.execute_reply":"2023-03-22T12:31:24.302633Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/car-diagnostics2/intents2..json\n/kaggle/input/car-diagnostics2/test.py\n/kaggle/input/car-diagnostics2/main.py\n/kaggle/input/car-diagnostics2/README.md\n/kaggle/input/car-diagnostics2/katana-assistant-data.pkl\n/kaggle/input/car-diagnostics2/app.py\n/kaggle/input/car-diagnostics2/requirements.txt\n/kaggle/input/car-diagnostics2/intents.json\n/kaggle/input/car-diagnostics2/tempCodeRunnerFile.py\n/kaggle/input/car-diagnostics2/data.pickle\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tflearn\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:24.312000Z","iopub.execute_input":"2023-03-22T12:31:24.312568Z","iopub.status.idle":"2023-03-22T12:31:36.404091Z","shell.execute_reply.started":"2023-03-22T12:31:24.312512Z","shell.execute_reply":"2023-03-22T12:31:36.402502Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tflearn in /opt/conda/lib/python3.7/site-packages (0.5.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tflearn) (9.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tflearn) (1.21.6)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tflearn) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\nimport tensorflow  \nimport nltk\nimport json\nfrom nltk.stem.lancaster import LancasterStemmer\nstemmer = LancasterStemmer()# things we need for Tensorflow\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\nimport pandas as pd\nimport pickle\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:36.409727Z","iopub.execute_input":"2023-03-22T12:31:36.410225Z","iopub.status.idle":"2023-03-22T12:31:40.491052Z","shell.execute_reply.started":"2023-03-22T12:31:36.410174Z","shell.execute_reply":"2023-03-22T12:31:40.489430Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/car-diagnostics2/intents.json\") as file:\n    data = json.load(file)\nwords = []\nclasses = []\ndocuments = []\nignore_words = ['?']\n# loop through each sentence in our intents patterns\nfor intent in data['intents']:\n    for pattern in intent['patterns']:\n        # tokenize each word in the sentence\n        w = nltk.word_tokenize(pattern)\n        # add to our words list\n        words.extend(w)\n        # add to documents in our corpus\n        documents.append((w, intent['tag']))\n        # add to our classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\n# stem and lower each word and remove duplicates\nwords = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\nwords = sorted(list(set(words)))# sort classes\nclasses = sorted(list(set(classes)))# documents = combination between patterns and intents\nprint (len(documents), \"documents\")\n# classes = intents\nprint (len(classes), \"classes\", classes)\n# words = all words, vocabulary\nprint (len(words), \"unique stemmed words\", words)            ","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:40.494355Z","iopub.execute_input":"2023-03-22T12:31:40.495190Z","iopub.status.idle":"2023-03-22T12:31:40.534476Z","shell.execute_reply.started":"2023-03-22T12:31:40.495143Z","shell.execute_reply":"2023-03-22T12:31:40.533094Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"77 documents\n20 classes ['Low engine oil level', 'Low fuel mileage', 'age', 'alignment', 'alternator', 'battery', 'engine overheating', 'engine_start', 'exhaust smoke', 'fuel_system', 'goodbye', 'greeting', 'greetings', 'help', 'hours', 'low ac cooling', 'squealing brakes', 'starter_motor', 'starting issue', 'suspension']\n122 unique stemmed words [\"'s\", 'a', 'ac', 'ag', 'air', 'altern', 'am', 'anym', 'ar', 'bad', 'battery', 'becaus', 'blow', 'brak', 'brandon', 'burn', 'but', 'bye', 'cao', 'car', 'charg', 'click', 'cold', 'com', 'crank', 'cya', 'day', 'dead', 'died', 'dim', 'do', 'doesnt', 'dont', 'driv', 'engin', 'exhaust', 'freeway', 'front', 'fuel', 'funny', 'get', 'going', 'good', 'goodby', 'greet', 'guy', 'hav', 'headlight', 'hello', 'help', 'hey', 'hi', 'hold', 'hot', 'hour', 'how', 'i', 'in', 'is', 'isnt', 'issu', 'it', 'keep', 'know', 'lat', 'leak', 'leav', 'left', 'level', 'light', 'lot', 'low', 'mak', 'misfir', 'mor', 'mot', 'my', \"n't\", 'nee', 'nois', 'not', 'of', 'oil', 'old', 'on', 'op', 'out', 'ov', 'overh', 'problem', 'pump', 'rid', 'right', 'rough', 'run', 'see', 'sid', 'smok', 'solenoid', 'sound', 'squ', 'stal', 'start', 'the', 'then', 'ther', 'to', 'try', 'turn', 'up', 'vibr', 'was', 'wast', 'what', 'when', 'whil', 'why', 'wo', 'work', 'ya', 'yo', 'you']\n","output_type":"stream"}]},{"cell_type":"code","source":"# create our training data\ntraining = []\n# create an empty array for our output\noutput_empty = [0] * len(classes)# training set, bag of words for each sentence\nfor doc in documents:\n    # initialize our bag of words\n    bag = []\n    # list of tokenized words for the pattern\n    pattern_words = doc[0]\n    # stem each word - create base word, in attempt to represent related words\n    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n    # create our bag of words array with 1, if word match found in current pattern\n    for w in words:\n        bag.append(1) if w in pattern_words else bag.append(0)\n    \n    # output is a '0' for each tag and '1' for current tag (for each pattern)\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n    \n    training.append([bag, output_row])# shuffle our features and turn into np.array\nrandom.shuffle(training)\ntraining = np.array(training)# create train and test lists. X - patterns, Y - intents\ntrain_x = list(training[:,0])\ntrain_y = list(training[:,1])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:40.538782Z","iopub.execute_input":"2023-03-22T12:31:40.539205Z","iopub.status.idle":"2023-03-22T12:31:40.559983Z","shell.execute_reply.started":"2023-03-22T12:31:40.539166Z","shell.execute_reply":"2023-03-22T12:31:40.558486Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n# equal to number of intents to predict output intent with softmax\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(train_y[0]), activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:40.563578Z","iopub.execute_input":"2023-03-22T12:31:40.564542Z","iopub.status.idle":"2023-03-22T12:31:40.932940Z","shell.execute_reply.started":"2023-03-22T12:31:40.564490Z","shell.execute_reply":"2023-03-22T12:31:40.931789Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Compile model. \n# Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\nsgd= SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:40.934332Z","iopub.execute_input":"2023-03-22T12:31:40.935638Z","iopub.status.idle":"2023-03-22T12:31:40.956912Z","shell.execute_reply.started":"2023-03-22T12:31:40.935588Z","shell.execute_reply":"2023-03-22T12:31:40.955312Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:31:40.958286Z","iopub.execute_input":"2023-03-22T12:31:40.958704Z","iopub.status.idle":"2023-03-22T12:31:50.556086Z","shell.execute_reply.started":"2023-03-22T12:31:40.958659Z","shell.execute_reply":"2023-03-22T12:31:50.554714Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/200\n16/16 [==============================] - 1s 3ms/step - loss: 3.0567 - accuracy: 0.0260\nEpoch 2/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.9712 - accuracy: 0.1169\nEpoch 3/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.8485 - accuracy: 0.1948\nEpoch 4/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.8194 - accuracy: 0.2208\nEpoch 5/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.6741 - accuracy: 0.2468\nEpoch 6/200\n16/16 [==============================] - 0s 3ms/step - loss: 2.5871 - accuracy: 0.3247\nEpoch 7/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.5194 - accuracy: 0.2987\nEpoch 8/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.3398 - accuracy: 0.2857\nEpoch 9/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.2839 - accuracy: 0.2987\nEpoch 10/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.1708 - accuracy: 0.3377\nEpoch 11/200\n16/16 [==============================] - 0s 2ms/step - loss: 2.0733 - accuracy: 0.4286\nEpoch 12/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.9593 - accuracy: 0.3896\nEpoch 13/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.8560 - accuracy: 0.4805\nEpoch 14/200\n16/16 [==============================] - 0s 3ms/step - loss: 1.5653 - accuracy: 0.5714\nEpoch 15/200\n16/16 [==============================] - 0s 3ms/step - loss: 1.4918 - accuracy: 0.5974\nEpoch 16/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.3901 - accuracy: 0.5844\nEpoch 17/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.3442 - accuracy: 0.5844\nEpoch 18/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.2601 - accuracy: 0.6494\nEpoch 19/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.3459 - accuracy: 0.5844\nEpoch 20/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.1401 - accuracy: 0.6104\nEpoch 21/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.0861 - accuracy: 0.6104\nEpoch 22/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.1554 - accuracy: 0.6623\nEpoch 23/200\n16/16 [==============================] - 0s 3ms/step - loss: 1.0049 - accuracy: 0.6623\nEpoch 24/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.9956 - accuracy: 0.6883\nEpoch 25/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.9010 - accuracy: 0.7143\nEpoch 26/200\n16/16 [==============================] - 0s 2ms/step - loss: 1.0893 - accuracy: 0.6883\nEpoch 27/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.7792\nEpoch 28/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.6459 - accuracy: 0.7922\nEpoch 29/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.8312\nEpoch 30/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.7349 - accuracy: 0.6753\nEpoch 31/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.8015 - accuracy: 0.6753\nEpoch 32/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.8442\nEpoch 33/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.7662\nEpoch 34/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.5967 - accuracy: 0.8182\nEpoch 35/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.6187 - accuracy: 0.8182\nEpoch 36/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.7662\nEpoch 37/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.5793 - accuracy: 0.8052\nEpoch 38/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8312\nEpoch 39/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.6479 - accuracy: 0.7792\nEpoch 40/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8831\nEpoch 41/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.5579 - accuracy: 0.8312\nEpoch 42/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.8312\nEpoch 43/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4517 - accuracy: 0.8701\nEpoch 44/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8831\nEpoch 45/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8312\nEpoch 46/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8831\nEpoch 47/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.6524 - accuracy: 0.7792\nEpoch 48/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.8831\nEpoch 49/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.8052\nEpoch 50/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4670 - accuracy: 0.8442\nEpoch 51/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8312\nEpoch 52/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4955 - accuracy: 0.8052\nEpoch 53/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8701\nEpoch 54/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.4055 - accuracy: 0.8442\nEpoch 55/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8571\nEpoch 56/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8182\nEpoch 57/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.9091\nEpoch 58/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8701\nEpoch 59/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8961\nEpoch 60/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.8312\nEpoch 61/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8442\nEpoch 62/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8442\nEpoch 63/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3620 - accuracy: 0.8831\nEpoch 64/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8701\nEpoch 65/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2795 - accuracy: 0.8961\nEpoch 66/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.9091\nEpoch 67/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8831\nEpoch 68/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3640 - accuracy: 0.8701\nEpoch 69/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.9221\nEpoch 70/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2367 - accuracy: 0.9221\nEpoch 71/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9351\nEpoch 72/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8442\nEpoch 73/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.9091\nEpoch 74/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9481\nEpoch 75/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.9091\nEpoch 76/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8961\nEpoch 77/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9221\nEpoch 78/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8442\nEpoch 79/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.8312\nEpoch 80/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.9091\nEpoch 81/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3743 - accuracy: 0.8831\nEpoch 82/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.9091\nEpoch 83/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8961\nEpoch 84/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2960 - accuracy: 0.8701\nEpoch 85/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8442\nEpoch 86/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.9221\nEpoch 87/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8961\nEpoch 88/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2343 - accuracy: 0.8831\nEpoch 89/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.8312\nEpoch 90/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2257 - accuracy: 0.9091\nEpoch 91/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8701\nEpoch 92/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8571\nEpoch 93/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9091\nEpoch 94/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2509 - accuracy: 0.9091\nEpoch 95/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.8961\nEpoch 96/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.8701\nEpoch 97/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8701\nEpoch 98/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2182 - accuracy: 0.9091\nEpoch 99/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8701\nEpoch 100/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2259 - accuracy: 0.9351\nEpoch 101/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.9091\nEpoch 102/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1681 - accuracy: 0.9351\nEpoch 103/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8442\nEpoch 104/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9091\nEpoch 105/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.8831\nEpoch 106/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.8831\nEpoch 107/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9091\nEpoch 108/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9091\nEpoch 109/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8571\nEpoch 110/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.9221\nEpoch 111/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.8701\nEpoch 112/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8571\nEpoch 113/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2187 - accuracy: 0.8831\nEpoch 114/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2432 - accuracy: 0.8961\nEpoch 115/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.8831\nEpoch 116/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.8961\nEpoch 117/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2213 - accuracy: 0.8831\nEpoch 118/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9221\nEpoch 119/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1491 - accuracy: 0.9351\nEpoch 120/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2142 - accuracy: 0.8961\nEpoch 121/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.8831\nEpoch 122/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1716 - accuracy: 0.8831\nEpoch 123/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8961\nEpoch 124/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.8701\nEpoch 125/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.8831\nEpoch 126/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2756 - accuracy: 0.8571\nEpoch 127/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1567 - accuracy: 0.9221\nEpoch 128/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.8961\nEpoch 129/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1952 - accuracy: 0.9221\nEpoch 130/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9870\nEpoch 131/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9221\nEpoch 132/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.8961\nEpoch 133/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.9221\nEpoch 134/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9221\nEpoch 135/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.8701\nEpoch 136/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2312 - accuracy: 0.9091\nEpoch 137/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8442\nEpoch 138/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1617 - accuracy: 0.9221\nEpoch 139/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1718 - accuracy: 0.9221\nEpoch 140/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2385 - accuracy: 0.8961\nEpoch 141/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9221\nEpoch 142/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2133 - accuracy: 0.8961\nEpoch 143/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1841 - accuracy: 0.9221\nEpoch 144/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9091\nEpoch 145/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9610\nEpoch 146/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.9091\nEpoch 147/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.9351\nEpoch 148/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.8831\nEpoch 149/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.8701\nEpoch 150/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2196 - accuracy: 0.9481\nEpoch 151/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9221\nEpoch 152/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9351\nEpoch 153/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1589 - accuracy: 0.9221\nEpoch 154/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.8701\nEpoch 155/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1698 - accuracy: 0.9610\nEpoch 156/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.8701\nEpoch 157/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1601 - accuracy: 0.9351\nEpoch 158/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.8831\nEpoch 159/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.9091\nEpoch 160/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1561 - accuracy: 0.8961\nEpoch 161/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.8442\nEpoch 162/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.8961\nEpoch 163/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9221\nEpoch 164/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9351\nEpoch 165/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9221\nEpoch 166/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.9221\nEpoch 167/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1729 - accuracy: 0.8961\nEpoch 168/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9091\nEpoch 169/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9351\nEpoch 170/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9091\nEpoch 171/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2078 - accuracy: 0.9351\nEpoch 172/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9091\nEpoch 173/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1441 - accuracy: 0.9481\nEpoch 174/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9221\nEpoch 175/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9221\nEpoch 176/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1987 - accuracy: 0.9091\nEpoch 177/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2078 - accuracy: 0.9221\nEpoch 178/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9351\nEpoch 179/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.8831\nEpoch 180/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1539 - accuracy: 0.9221\nEpoch 181/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.8701\nEpoch 182/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2304 - accuracy: 0.8831\nEpoch 183/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.9091\nEpoch 184/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.8701\nEpoch 185/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2056 - accuracy: 0.8831\nEpoch 186/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1702 - accuracy: 0.9091\nEpoch 187/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9091\nEpoch 188/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1621 - accuracy: 0.9221\nEpoch 189/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1321 - accuracy: 0.9091\nEpoch 190/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.8701\nEpoch 191/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.8571\nEpoch 192/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.8961\nEpoch 193/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2844 - accuracy: 0.8831\nEpoch 194/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9351\nEpoch 195/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.8831\nEpoch 196/200\n16/16 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9091\nEpoch 197/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9351\nEpoch 198/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.8961\nEpoch 199/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.8701\nEpoch 200/200\n16/16 [==============================] - 0s 2ms/step - loss: 0.1617 - accuracy: 0.9091\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f1ad43fbf90>"},"metadata":{}}]},{"cell_type":"code","source":"with open('car-diagnostic.pkl', 'wb') as f:\n    pickle.dump(model, f)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:41:22.571290Z","iopub.execute_input":"2023-03-22T12:41:22.571794Z","iopub.status.idle":"2023-03-22T12:41:22.649573Z","shell.execute_reply.started":"2023-03-22T12:41:22.571751Z","shell.execute_reply":"2023-03-22T12:41:22.648189Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dropout\n.........vars\n......dropout_1\n.........vars\n...metrics\n......mean\n.........vars\n............0\n............1\n......mean_metric_wrapper\n.........vars\n............0\n............1\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2023-03-22 12:41:22         2335\nvariables.h5                                   2023-03-22 12:41:22       122808\nmetadata.json                                  2023-03-22 12:41:22           64\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_up_sentence(sentence):\n    # tokenize the pattern - split words into array\n    sentence_words = nltk.word_tokenize(sentence)\n    # stem each word - create short form for word\n    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n    return sentence_words# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\ndef bow(sentence, words, show_details=True):\n    # tokenize the pattern\n    sentence_words = clean_up_sentence(sentence)\n    # bag of words - matrix of N words, vocabulary matrix\n    bag = [0]*len(words)  \n    for s in sentence_words:\n        for i,w in enumerate(words):\n            if w == s: \n                # assign 1 if current word is in the vocabulary position\n                bag[i] = 1\n                if show_details:\n                    print (\"found in bag: %s\" % w)\n    return(np.array(bag))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:32:01.755178Z","iopub.execute_input":"2023-03-22T12:32:01.755716Z","iopub.status.idle":"2023-03-22T12:32:01.765324Z","shell.execute_reply.started":"2023-03-22T12:32:01.755666Z","shell.execute_reply":"2023-03-22T12:32:01.763711Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def classify_local(sentence):\n    ERROR_THRESHOLD = 0.25\n    \n    # generate probabilities from the model\n    input_data = pd.DataFrame([bow(sentence, words)], dtype=float, index=['input'])\n    results = model.predict([input_data])[0]\n    # filter out predictions below a threshold, and provide intent index\n    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n    # sort by strength of probability\n    results.sort(key=lambda x: x[1], reverse=True)\n    return_list = []\n    for r in results:\n        return_list.append((classes[r[0]], str(r[1])))\n    # return tuple of intent and probability\n    \n    return return_list","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:32:08.084244Z","iopub.execute_input":"2023-03-22T12:32:08.084759Z","iopub.status.idle":"2023-03-22T12:32:08.094001Z","shell.execute_reply.started":"2023-03-22T12:32:08.084712Z","shell.execute_reply":"2023-03-22T12:32:08.092466Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"classify_local(\"Car Brakes don't work\")","metadata":{"execution":{"iopub.status.busy":"2023-03-22T12:32:12.558495Z","iopub.execute_input":"2023-03-22T12:32:12.559527Z","iopub.status.idle":"2023-03-22T12:32:12.761645Z","shell.execute_reply.started":"2023-03-22T12:32:12.559462Z","shell.execute_reply":"2023-03-22T12:32:12.760160Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"found in bag: car\nfound in bag: brak\nfound in bag: do\nfound in bag: n't\nfound in bag: work\n1/1 [==============================] - 0s 109ms/step\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[('squealing brakes', '0.35313186')]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}